{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw4Jpi4S4_JA"
   },
   "source": [
    "# Coding Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu9uKue79Ab7"
   },
   "source": [
    "## CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-p69dVJd5DRa"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L_A5TfhD5LEb"
   },
   "outputs": [],
   "source": [
    "def ctc_loss(log_probs):\n",
    "    \"\"\"\n",
    "    Compute CTC loss based on the log-probability matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prob = np.log(log_probs)\n",
    "    # print(-final_prob)\n",
    "    loss = sum(-final_prob)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r3bV-UpD5LoV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.4185809  4.77952357 7.26443022 8.51719319]\n"
     ]
    }
   ],
   "source": [
    "log_probs = np.array([\n",
    "    [0.6, 0.2, 0.1, 0.1],\n",
    "    [0.1, 0.7, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.7, 0.1],\n",
    "    [0.1, 0.6, 0.1, 0.2],\n",
    "])\n",
    "\n",
    "loss = ctc_loss(log_probs)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0Ag464N9C-Z"
   },
   "source": [
    "## Greedy Decoding\n",
    "**You are NOT allowed to use np.argmax or any similar function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "753pe2bC9ET8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded word: ABC\n"
     ]
    }
   ],
   "source": [
    "def greedy_ctc_decode(probs, idx2char):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding:\n",
    "      - probs: list of lists, shape (T, V)\n",
    "      - idx2char: dict mapping index -> character\n",
    "    \"\"\"\n",
    "    # T, V = probs.shape\n",
    "    S = len(idx2char)\n",
    "    # 1) Argmax over classes at each timestep\n",
    "    best_path = []\n",
    "    for t in range(len(probs)):\n",
    "        for v in range(len(probs[t])):\n",
    "            if probs[t][v-1] < probs[t][v]: # probs\n",
    "                best_path.append(v)\n",
    "    char = []\n",
    "    for i in best_path:\n",
    "        char.append(idx2char[i])\n",
    "    print(char)\n",
    "    # 2) Collapse repeats and remove blanks\n",
    "    char_no_blank_or_repeat = []\n",
    "    for i in range(len(char)):\n",
    "        if char[i] != '_' and char[i] != char[i-1]:\n",
    "            char_no_blank_or_repeat.append(char[i])\n",
    "\n",
    "    # 3) Map indices to characters\n",
    "\n",
    "        \n",
    "\n",
    "    # 4) Join characters and return the predicted word\n",
    "    \n",
    "\n",
    "    return  \"\".join(char_no_blank_or_repeat)\n",
    "\n",
    "# Vocabulary:\n",
    "# 0 = blank '_'\n",
    "# 1 = 'A'\n",
    "# 2 = 'B'\n",
    "# 3 = 'C'\n",
    "idx2char = {\n",
    "    0: \"_\",\n",
    "    1: \"A\",\n",
    "    2: \"B\",\n",
    "    3: \"C\"\n",
    "}\n",
    "\n",
    "# probs[t][v] = probability of symbol v at time t\n",
    "probs = [\n",
    "    [0.1, 0.6, 0.2, 0.1],  # t0 -> 'A'\n",
    "    [0.1, 0.7, 0.1, 0.1],  # t1 -> 'A'\n",
    "    [0.5, 0.3, 0.1, 0.1],  # t2 -> blank\n",
    "    [0.1, 0.1, 0.7, 0.1],  # t3 -> 'B'\n",
    "    [0.1, 0.1, 0.7, 0.1],  # t4 -> 'B'\n",
    "    [0.2, 0.1, 0.1, 0.6],  # t5 -> 'C'\n",
    "]\n",
    "\n",
    "decoded = greedy_ctc_decode(probs, idx2char)\n",
    "print(\"Decoded word:\", decoded)\n",
    "# idx2char.items()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1kMq0Kz5ADC"
   },
   "source": [
    "# Discusison Quesiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4o1216V9xuv"
   },
   "source": [
    "1. because we just take the summation of negative log of probability of given symbol at time Xt, because they represent what is the most liklehood of a symbol happening at time t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5attJwp9y2J"
   },
   "source": [
    "2. A blank token is not enough for sentences as we need to account for spaces between words. We should introduce a space token in addition to the blank token. The code modifications would involve updating the vocabulary to include the space character and ensuring that the decoding process interprets spaces between words when collapsing repeats and removing blanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. yes, tha bsic one is the greedy decoding, which takes the best prob at each time step.\n",
    "the other one is the viterbi-like beam search CTC, which searches with k beams for all possible paths which is much slower but with more stable liklehood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h1kMq0Kz5ADC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
