# High Fidelity Neural Audio Compression: Complete Deep Dive

## Part 1: The Problem Statement (Mathematical Foundation)

### 1.1 Audio Compression Fundamentals

**Audio Signal Representation**

An audio signal is a continuous wave that gets sampled at regular intervals:

```
Continuous signal: x(t) where t âˆˆ â„
Sampled signal: x[n] where n âˆˆ â„¤, sampled at rate fs (44.1 kHz or 48 kHz)
```

For a stereo audio file with duration T seconds at sample rate fs:

- Number of samples: N = fs Ã— T
- Bitrate (uncompressed): B_uncompressed = fs Ã— bits_per_sample Ã— channels
- Example: 48 kHz, 16 bits, stereo = 48000 Ã— 16 Ã— 2 = 1,536,000 bits/second

**The Compression Goal**

We want to reduce bitrate from B_uncompressed to B_compressed while maintaining perceptual quality.

Compression ratio: CR = B_uncompressed / B_compressed

For example: 1,536,000 bits/sec â†’ 24,000 bits/sec = CR of 64x

**Rate-Distortion Theory** (Shannon, 1959)

The fundamental trade-off in compression is:

```
R(D) = min I(X; Y) 
       subject to E[d(X,Y)] â‰¤ D
```

Where:

- R(D) = minimum bitrate required
- I(X; Y) = mutual information between original X and compressed Y
- d(X,Y) = distortion measure (perceptual loss)
- D = acceptable distortion threshold

**Key Insight**: There's a fundamental limit to how much you can compress while maintaining a certain quality level.

### 1.2 Why Neural Networks?

Traditional audio codecs (MP3, AAC, Opus) use:

1. Fourier transform to convert to frequency domain
2. Hand-crafted rules about human hearing (threshold, masking)
3. Huffman or arithmetic coding

**Problem**: These rules are general, not optimized for:

- Specific audio types (music vs. speech vs. ambient)
- Temporal dependencies (what sounds likely to come next)
- Perceptual importance of specific frequency combinations

**Neural Network Advantage**: Can learn optimal compression for the specific distribution of data it trains on.

---

## Part 2: Architecture in Detail

### 2.1 End-to-End System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ENCODING PHASE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Audio                Latent Code                         â”‚
â”‚  x[n] âˆˆ â„^N          z_q[m] âˆˆ â„¤^M                              â”‚
â”‚   â”‚                         â”‚                                   â”‚
â”‚   â–¼                         â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚   Encoder    â”‚      â”‚  Quantizer   â”‚                        â”‚
â”‚  â”‚ (neural net) â”‚â”€â”€â”€â”€â”€â–¶â”‚  (round to   â”‚â”€â”€â”€â”€â–¶ Bitstream         â”‚
â”‚  â”‚              â”‚      â”‚   integers)  â”‚      (compressed)       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚      (strided                                                   â”‚
â”‚    convolutions)                                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DECODING PHASE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Bitstream              Latent Code          Output Audio       â”‚
â”‚  (received)             z_q[m]               y[n] â‰ˆ x[n]       â”‚
â”‚   â”‚                        â”‚                    â”‚               â”‚
â”‚   â–¼                        â–¼                    â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Entropy Decoder                 â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  Decoder     â”‚   â”‚
â”‚  â”‚  (reverse of encoder + quantizer)â”‚      â”‚ (neural net) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  (transposed    â”‚
â”‚                                               convolutions)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 The Encoder Network (Analysis)

**Input**: Raw audio x[n] with shape (batch_size, num_samples)

**Architecture** (simplified, actual paper uses residual blocks):

```
Layer 1: Conv1D(out_channels=128, kernel_size=7, stride=2, padding=3)
         Output: (batch, 128, samples/2)
         
Layer 2: Conv1D(out_channels=256, kernel_size=7, stride=2, padding=3)
         Output: (batch, 256, samples/4)
         
Layer 3: Conv1D(out_channels=512, kernel_size=7, stride=2, padding=3)
         Output: (batch, 512, samples/8)
         
...more layers...

Final:   Conv1D(out_channels=num_latent_channels, kernel_size=7, stride=1)
         Output: z[m] (batch, num_latent_channels, compressed_length)
```

**Mathematical Details**:

A strided convolution operation:

```
For input x[n] and kernel h[k] with stride s:

z[m] = Î£(k=0 to K-1) h[k] Ã— x[mÃ—s + k]

Where:
- m = 0, 1, 2, ...
- K = kernel size
- s = stride

Example (stride=2):
z[0] = h[0]Ã—x[0] + h[1]Ã—x[1] + ... + h[K-1]Ã—x[K-1]
z[1] = h[0]Ã—x[2] + h[1]Ã—x[3] + ... + h[K-1]Ã—x[K+1]
z[2] = h[0]Ã—x[4] + h[1]Ã—x[5] + ... + h[K-1]Ã—x[K+3]
```

**Compression Ratio from Encoder**:

If input has N samples and output has M latent codes:

- Compression from encoder alone: N / M (typically 4x-8x)

### 2.3 The Quantizer

**The Problem**: Real numbers can't be compressed efficiently. We need discrete values.

**Solution**: Round continuous latents to discrete integers.

**Straight-Through Estimator (STE)**:

During training, we need gradients, but rounding has no gradient. Solution:

```
Forward pass (quantization):
z_q[m] = round(z[m])

Backward pass (gradient):
dL/dz[m] = dL/dz_q[m]  (treat rounding as identity function)
```

Mathematically:

```
quantize(z) = round(z) + (z - round(z))
               â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               discrete   gradient path
               output     (straight-through)
```

**Information-Theoretic View**:

After quantization, each latent code needs logâ‚‚(num_levels) bits to encode.

If z_q[m] âˆˆ [-128, 127] (8-bit quantization):

- Each value needs 8 bits
- M latent codes need 8M bits total

**Entropy Coding** (comes after quantizer):

Instead of using exactly 8 bits per code, use variable-length codes:

- Common codes (like 0) â†’ 2 bits
- Rare codes (like 127) â†’ 12 bits

This uses entropy encoding (Huffman, arithmetic coding):

```
Entropy H = Î£ -p(z_q) Ã— logâ‚‚(p(z_q))

Where p(z_q) = probability of code z_q appearing
```

---

## Part 3: The Decoder (Synthesis)

**Input**: z_q[m] (quantized latent codes)

**Architecture** (reverse of encoder):

```
Layer 1: ConvTranspose1D(out_channels=512, kernel_size=7, stride=2, padding=3)
         Output: (batch, 512, samples/4)
         
Layer 2: ConvTranspose1D(out_channels=256, kernel_size=7, stride=2, padding=3)
         Output: (batch, 256, samples/2)
         
Layer 3: ConvTranspose1D(out_channels=128, kernel_size=7, stride=2, padding=3)
         Output: (batch, 128, samples)
         
Final:   ConvTranspose1D(out_channels=1, kernel_size=7, stride=1)
         Output: y[n] (batch, samples)
```

**Transposed Convolution (Deconvolution)**:

```
For output y and input z, with stride s and kernel h:

y[n] = Î£(m: mÃ—s â‰¤ n < (m+1)Ã—s) h[n - mÃ—s] Ã— z[m]

This "upsamples" the signal back to original size
```

---

## Part 4: Training Process (The Learning Algorithm)

### 4.1 Loss Function

The total loss balances three objectives:

```
L_total = L_recon + Î² Ã— L_rate + Î³ Ã— L_commit

Where:
- L_recon = reconstruction loss (how close is output to input)
- L_rate = rate loss (how small is the compressed file)
- L_commit = codebook commitment loss
- Î², Î³ = hyperparameters (weights)
```

### 4.2 Reconstruction Loss

Measures how different the reconstructed audio y[n] is from original x[n].

**Option 1: Mean Squared Error (MSE)**

```
L_recon(MSE) = (1/N) Ã— Î£(n=0 to N-1) (y[n] - x[n])Â²

Problem: Doesn't match human perception (doesn't account for 
perceptual importance of different frequencies)
```

**Option 2: Multi-Scale STFT Loss** (used in this paper)

STFT = Short-Time Fourier Transform

```
STFT_x[t, f] = Î£(n) x[n] Ã— window(n) Ã— e^(-j2Ï€fn/N)

Where:
- t = time frame index
- f = frequency bin
- j = imaginary unit
- N = FFT size
- window(n) = Hann window or similar

Multi-Scale STFT Loss:
L_recon(STFT) = Î£(scales) { 
                  |log|STFT_x| - log|STFT_y|| +
                  |âˆ STFT_x - âˆ STFT_y|
                }

Where:
- |Â·| = magnitude
- âˆ Â· = phase
- Multiple scales = different window sizes (512, 1024, 2048, ...)
```

**Why STFT is better**:

- Captures frequency content (humans hear frequencies)
- Multi-scale accounts for different time-frequency resolutions
- log magnitude emphasizes perceptual differences

### 4.3 Rate Loss

Encourages the encoder to produce codes that compress well.

```
L_rate = -Î£(m=0 to M-1) logâ‚‚(p(z_q[m] | training_data))

This is the cross-entropy loss from the learned entropy model.
The network learns to assign higher probability to codes it uses.
```

**Entropy Model**:

A separate neural network learns the distribution p(z_q):

```
p_model(z_q) = learned probability distribution over codes

For example, might learn:
p(0) = 0.3  (code 0 appears 30% of time)
p(1) = 0.2  (code 1 appears 20% of time)
p(2) = 0.1  (code 2 appears 10% of time)
...

Actual bits needed = -logâ‚‚(p) 
Code 0 needs: -logâ‚‚(0.3) â‰ˆ 1.74 bits
Code 1 needs: -logâ‚‚(0.2) â‰ˆ 2.32 bits
Code 2 needs: -logâ‚‚(0.1) â‰ˆ 3.32 bits
```

### 4.4 Codebook Commitment Loss

For vector quantization (if used):

```
L_commit = Î² Ã— ||z - sg[z_q]||Â² + ||sg[z] - z_q||Â²

Where sg[Â·] = stop_gradient (don't backprop through this)

This helps:
1. Encoder stay close to actual quantized values
2. Quantized values stay close to encoder outputs
```

### 4.5 Complete Training Algorithm

```
INPUT: Training data X = {xâ‚, xâ‚‚, ..., x_B} (batch of audio)
PARAMETERS: Encoder E_Î¸, Decoder D_Ï†, Entropy Model p_Ïˆ

FOR each training iteration:
  
  # Forward pass
  z = E_Î¸(x)                              // Encode
  z_q = quantize(z)                       // Quantize with STE
  y = D_Ï†(z_q)                            // Decode
  p = p_Ïˆ(z_q)                            // Get probabilities
  
  # Compute losses
  L_recon = STFT_loss(x, y)               // Reconstruction
  L_rate = -mean(logâ‚‚(p))                 // Rate
  L_commit = ||z - sg[z_q]||Â² + ||sg[z] - z_q||Â²
  
  L_total = L_recon + Î²Ã—L_rate + Î³Ã—L_commit
  
  # Backward pass
  Î¸' = Î¸ - Î± Ã— âˆ‡_Î¸ L_total                // Update encoder
  Ï†' = Ï† - Î± Ã— âˆ‡_Ï† L_total                // Update decoder
  Ïˆ' = Ïˆ - Î± Ã— âˆ‡_Ïˆ L_total                // Update entropy model
  
  # Update parameters
  Î¸ â† Î¸'
  Ï† â† Ï†'
  Ïˆ â† Ïˆ'

END FOR
```

---

## Part 5: Information Flow Diagrams

### 5.1 Signal Processing Flow

```
TRAINING TIME:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input Audio x[n]     (48 kHz, 16-bit)        [samples: 240,000]
    â”‚
    â”‚ [Encoder Network - 3 strided convs]
    â”‚ - stride=2, stride=2, stride=2
    â”‚ - Total downsampling: 8x
    â–¼
Latent Code z[m]                             [samples: 30,000]
    â”‚
    â”‚ [Straight-Through Quantizer]
    â”‚ - Round to integers
    â–¼
Quantized z_q[m]     (integer values)        [samples: 30,000]
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚                  â”‚
    â–¼      â–¼                  â–¼
  [Decoder]  [Entropy Model]  [Loss Functions]
    â”‚          â”‚                  â”‚
    â–¼          â–¼                  â–¼
Recon y[n]  p(z_q)          L_total
    â”‚          â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
            [Backward Pass / Gradients]
                      â”‚
                      â–¼
            Update all parameters


INFERENCE TIME:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input Audio x[n]
    â”‚
    â–¼
  [Encoder] â†’ z
    â”‚
    â–¼
  [Quantizer] â†’ z_q (integers)
    â”‚
    â–¼
  [Entropy Coder] â†’ Bitstream (compressed)
    â”‚
    â”‚ (transmit or store)
    â”‚
    â–¼
  [Entropy Decoder] â†’ z_q (recover integers)
    â”‚
    â–¼
  [Decoder] â†’ y[n]
    â”‚
    â–¼
Output Audio y[n] (reconstructed)
```

### 5.2 Detailed Encoder Layer-by-Layer Transformation

```
Audio Waveform (Time Domain):
â”‚
â”‚ Sample values for one second at 48 kHz:
â”‚ [0.1, -0.05, 0.08, 0.02, -0.12, 0.09, ...]  (48,000 values)
â”‚
â–¼

Conv1D(stride=2):
â”‚ Each output value is combination of 7 consecutive input values
â”‚ Output length: 48,000 / 2 = 24,000
â”‚ Output channels: 128
â”‚ [0.34, -0.18, 0.22, ...] (24,000 values, 128 channels)
â”‚
â–¼

Conv1D(stride=2):
â”‚ Output length: 24,000 / 2 = 12,000
â”‚ Output channels: 256
â”‚ [0.51, -0.23, 0.17, ...] (12,000 values, 256 channels)
â”‚
â–¼

Conv1D(stride=2):
â”‚ Output length: 12,000 / 2 = 6,000
â”‚ Output channels: 512
â”‚ [0.67, -0.41, 0.39, ...] (6,000 values, 512 channels)
â”‚
â–¼

Residual Blocks (maintain 6,000 length, learn features):
â”‚ [0.72, -0.38, 0.44, ...]
â”‚
â–¼

Final Conv1D:
â”‚ Output length: 6,000
â”‚ Output channels: num_latent_channels (e.g., 64)
â”‚ Latent Code: z[m] âˆˆ â„^6000Ã—64
â”‚
â”‚ Compression: 48,000 â†’ 6,000 = 8x reduction
â”‚ (or 48,000Ã—16 bits â†’ 6,000Ã—32 bits in terms of information)
```

---

## Part 6: Information-Theoretic Analysis

### 6.1 Bits Required After Quantization

```
Without entropy coding:
Total bits = M Ã— logâ‚‚(num_quantization_levels)

Example:
- M = 6,000 latent codes
- 8-bit quantization (256 levels)
- Total bits = 6,000 Ã— 8 = 48,000 bits

Bitrate = 48,000 bits / 1 second = 48 kbps

Original bitrate = 48,000 samples Ã— 16 bits = 768 kbps

Compression ratio = 768 / 48 = 16x
```

### 6.2 With Entropy Coding (Arithmetic/Huffman)

The entropy model learns p(z_q). Using arithmetic coding:

```
For a sequence z_q[0], z_q[1], ..., z_q[M-1]:

Actual bits used â‰ˆ M Ã— H(Z_q)

Where H(Z_q) = entropy = -Î£ p(z) Ã— logâ‚‚(p(z))

Example distribution learned:
z_q=0:   p=0.30  â†’ -logâ‚‚(0.30) = 1.74 bits
z_q=1:   p=0.20  â†’ -logâ‚‚(0.20) = 2.32 bits
z_q=2:   p=0.15  â†’ -logâ‚‚(0.15) = 2.74 bits
z_q=3:   p=0.10  â†’ -logâ‚‚(0.10) = 3.32 bits
z_q=4:   p=0.10  â†’ -logâ‚‚(0.10) = 3.32 bits
z_q=5:   p=0.10  â†’ -logâ‚‚(0.10) = 3.32 bits
z_q=6:   p=0.05  â†’ -logâ‚‚(0.05) = 4.32 bits

H = 0.30Ã—1.74 + 0.20Ã—2.32 + ... + 0.05Ã—4.32
  â‰ˆ 2.60 bits average per code

Total bits = 6,000 Ã— 2.60 â‰ˆ 15,600 bits â‰ˆ 15.6 kbps

Compression ratio = 768 / 15.6 â‰ˆ 49x
```

---

## Part 7: Complete Training Loop Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPOCH 1: Random initialized network (bad compression)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Encoder: [untrained]  â†’ Latent codes are random           â”‚
â”‚  Decoder: [untrained]  â†’ Output is noise                   â”‚
â”‚                                                             â”‚
â”‚  L_recon = 1.5 (MSE between noise and audio)               â”‚
â”‚  L_rate = 5.2 (codes don't compress well)                  â”‚
â”‚  L_total = 1.5 + Î²Ã—5.2 + Î³Ã—... = very high                â”‚
â”‚                                                             â”‚
â”‚  Gradient computed, parameters updated                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (after 1000 iterations)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPOCH 100: Network learning structure                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Encoder: [learning freq patterns]                         â”‚
â”‚  Decoder: [recreating rough shapes]                        â”‚
â”‚                                                             â”‚
â”‚  L_recon = 0.3 (output somewhat recognizable)              â”‚
â”‚  L_rate = 3.1 (entropy model improving)                    â”‚
â”‚  L_total = 0.3 + 3.1Ã—Î² + ... = decreasing                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (after 50,000 iterations)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPOCH 1000: Network converged (good compression)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Encoder: [learned to extract key features]                â”‚
â”‚  Decoder: [reconstructs perceptually similar audio]        â”‚
â”‚                                                             â”‚
â”‚  L_recon = 0.08 (output almost identical to input)         â”‚
â”‚  L_rate = 1.8 (high compression, good distribution)        â”‚
â”‚  L_total = 0.08 + 1.8Ã—Î² + ... â‰ˆ minimal                    â”‚
â”‚                                                             â”‚
â”‚  Output: Audio compressed by 50x, sounds identical!        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 8: Why This Works (Information Theory)

### 8.1 Learned Representation

The encoder learns to extract:

```
z[m] = [
  f_bass[m],         // How much bass in this frame
  f_mids[m],         // How much midrange
  f_treble[m],       // How much high-frequency
  f_brightness[m],   // Perceptual brightness
  f_dynamic[m],      // Dynamic range
  f_temporal[m],     // Temporal changes
  ...
]

These are learned features, not hand-designed!
```

### 8.2 Compression Works Because:

```
Original audio has high entropy:
H(x) â‰ˆ 16 bits/sample (16-bit PCM has high uncertainty)

Latent codes have lower entropy:
H(z) â‰ˆ 3-5 bits/code (much more predictable)

Why? Because the encoder learns that most audio samples 
can be predicted from nearby samples and previous frames.

The network captures dependencies:
- Temporal: x[n] depends strongly on x[n-1], x[n-2], ...
- Frequency: High frequencies have patterns with low frequencies
- Perceptual: Some information doesn't matter to human hearing

By learning these dependencies, entropy drops!

Shannon's Source Coding Theorem:
bits_needed â‰¥ H(source)

After encoder:
bits_needed(z) << bits_needed(x)
Because H(z) << H(x)
```

---

## Part 9: Key Innovations in the Paper

### 9.1 Residual Vector Quantization (RVQ)

Instead of one quantization step, use multiple steps on residuals:

```
z = E(x)                    // Initial latent
zâ‚_q = quantize(z)          // First quantization (coarse)
residualâ‚ = z - zâ‚_q        // What was lost

residualâ‚_q = quantize(residualâ‚)  // Quantize the loss
residualâ‚‚ = residualâ‚ - residualâ‚_q

residualâ‚‚_q = quantize(residualâ‚‚)  // Quantize remaining loss
...

Final: z_total = zâ‚_q + residualâ‚_q + residualâ‚‚_q + ...

Benefit: Multi-scale quantization, better quality
Decoder gets progressively better approximations
```

### 9.2 Factorized Entropy Model

Entropy model uses chain rule:

```
p(z_q[0, 0], z_q[0, 1], ..., z_q[M-1, C-1]) 
= p(z_q[0, 0]) Ã— p(z_q[0, 1] | z_q[0, 0]) Ã— ...

But approximates as factorized:
â‰ˆ Î  p(z_q[m, c])

Where m = position index, c = channel index

This is more tractable computationally while still 
capturing important information about code probabilities.
```

---

## Part 10: Deep Practice Questions & Answers

### Question 1: Strided Convolution Math

**Q: If input has 48,000 samples and we apply Conv1D with kernel_size=7, stride=2, padding=3, what's the output length?**

Formula:

```
output_length = floor((input_length + 2Ã—padding - kernel_size) / stride) + 1
```

**A:**

```
output_length = floor((48,000 + 2Ã—3 - 7) / 2) + 1
              = floor((48,000 + 6 - 7) / 2) + 1
              = floor(47,999 / 2) + 1
              = floor(23,999.5) + 1
              = 23,999 + 1
              = 24,000

So input of 48,000 â†’ output of 24,000 (2x compression)
```

---

### Question 2: Bitrate Calculation

**Q: If we have 6,000 latent codes and entropy is 3.5 bits per code, and audio is 5 seconds long, what's the final bitrate in kbps?**

**A:**

```
Total bits = num_codes Ã— entropy_per_code
           = 6,000 Ã— 3.5
           = 21,000 bits

Duration = 5 seconds

Bitrate = 21,000 bits / 5 seconds = 4,200 bits/second = 4.2 kbps

Original bitrate (48 kHz, 16-bit, mono):
= 48,000 Ã— 16 = 768 kbps

Compression ratio = 768 / 4.2 â‰ˆ 183x
```

---

### Question 3: Loss Function Understanding

**Q: Explain what each term in the loss means:**

```
L_total = L_recon + Î²Ã—L_rate + Î³Ã—L_commit
```

**A:**

1. **L_recon (reconstruction loss)**:
    
    - Measures: How close is reconstructed audio to original?
    - Formula: Multi-scale STFT magnitude and phase difference
    - Goal: Minimize to keep high perceptual quality
    - Unit: dB or normalized scale
2. **L_rate (rate loss)**:
    
    - Measures: How many bits does this compressed signal need?
    - Formula: -Î£ logâ‚‚(p(z_q))
    - Goal: Minimize to reduce file size
    - Unit: bits per code
    - Î²: Weight that controls compression aggressiveness
3. **L_commit (codebook commitment)**:
    
    - Measures: How close is encoder output to quantized values?
    - Formula: ||z - sg[z_q]||Â² + ||sg[z] - z_q||Â²
    - Goal: Prevent encoder from drifting away from actual codes
    - Unit: squared error
    - Î³: Weight for this constraint

---

### Question 4: Quantization and Gradients

**Q: Why do we need Straight-Through Estimator? What's the problem with direct quantization during backprop?**

**A:**

The rounding function round(x) has:

- Forward: z_q = round(z) (discrete output) âœ“
- Gradient: dround/dz = 0 everywhere (gradient is zero) âœ—

Problem:

```
If dL/dz_q = 0.5 (we want to update z)
Then dL/dz = dL/dz_q Ã— (dz_q/dz)
           = 0.5 Ã— 0
           = 0 (no gradient!)

Network can't learn!
```

Solution - Straight-Through Estimator:

```
During forward pass:
z_q = round(z)  [use actual quantization]

During backward pass (manually defined):
dL/dz = dL/dz_q [treat as identity function]
```

Mathematically, we define:

```
quantize(z) = round(z) + (z - round(z))
              â†‘           â†‘
              discrete    gradient path
              part        (continuous)
```

This allows gradient to flow backward!

---

### Question 5: Entropy Coding

**Q: If a code value appears with probability p=0.1, how many bits does it need using optimal entropy coding?**

**A:**

Optimal entropy coding assigns:

```
bits_needed = -logâ‚‚(p)

For p = 0.1:
bits_needed = -logâ‚‚(0.1)
            = -logâ‚‚(1/10)
            = -(-logâ‚‚(10))
            = logâ‚‚(10)
            â‰ˆ 3.32 bits

So this code takes 3.32 bits (or round to 4 bits in practice)

Compare:
- Frequent code (p=0.3): -logâ‚‚(0.3) â‰ˆ 1.74 bits
- Rare code (p=0.01): -logâ‚‚(0.01) â‰ˆ 6.64 bits

Average: H = 0.3Ã—1.74 + 0.01Ã—6.64 + ...
```

---

### Question 6: STFT Loss Function

**Q: Why is Multi-Scale STFT Loss better than Mean Squared Error (MSE)?**

**A:**

MSE problem:

```
L_MSE = (1/N) Î£(n=0 to N-1) (y[n] - x[n])Â²

If original: [1.0, 0.0, 1.0, 0.0, ...]  (amplitude 1.0)
If recon:    [0.9, 0.1, 0.9, 0.1, ...]  (amplitude 0.9)

MSE = 0.5 Ã— (0.1Â² + 0.1Â² + ...) = 0.005

But to human ear, this sounds very similar!
MSE doesn't match human perception.
```

STFT Loss advantage:

```
Captures frequency content:
STFT_x[t, f] = Î£(n) x[n] Ã— window(n) Ã— e^(-j2Ï€fn/N)

Compares magnitude and phase separately:
L = |log|STFT_x[t,f]| - log|STFT_y[t,f]|| + |âˆ STFT_x - âˆ STFT_y|

Why this works:
- log magnitude: humans perceive amplitude logarithmically
- Phase: important for transients and perception
- Multi-scale: different window sizes (512, 1024, 2048)
  catches both fine details and broad structures

Example:
Two signals with same MSE but different STFT:
- MSE might be identical
- But one might be missing bass frequencies
- STFT loss catches this difference!
```

---

### Question 7: Compression Ratio Calculation (Complete Example)

**Q: A stereo audio file has:**

- Duration: 10 seconds
- Sample rate: 48 kHz
- Original bit depth: 16 bits
- After compression: 6,000 latent codes with entropy 2.5 bits

**Calculate the compression ratio.**

**A:**

**Original file size:**

```
Samples per channel = 48,000 Hz Ã— 10 sec = 480,000 samples
Bits per sample = 16 bits
Channels = 2 (stereo)

Original bits = 480,000 Ã— 16 Ã— 2 = 15,360,000 bits
Original bitrate = 15,360,000 / 10 = 1,536 kbps
Original file = 15,360,000 / 8 = 1,920 KB â‰ˆ 1.92 MB
```

**Compressed file size:**

```
Latent codes = 6,000 per second (total: 60,000 for 10 sec)
Entropy per code = 2.5 bits

Compressed bits = 60,000 Ã— 2.5 = 150,000 bits
Compressed bitrate = 150,000 / 10 = 15 kbps
Compressed file = 150,000 / 8 = 18,750 bytes â‰ˆ 18.75 KB
```

**Compression Ratio:**

```
CR = original / compressed
   = 15,360,000 / 150,000
   = 102.4x

Or in bitrate:
CR = 1,536 kbps / 15 kbps
   = 102.4x

Or in file size:
CR = 1.92 MB / 0.01875 MB
   = 102.4x
```

---

### Question 8: Encoder Downsampling

**Q: An encoder uses 3 Conv1D layers with stride=2 each. Starting with 48,000 samples, what's the output length?**

**A:**

```
Layer 1: stride=2
Outputâ‚ = 48,000 / 2 = 24,000 samples

Layer 2: stride=2
Outputâ‚‚ = 24,000 / 2 = 12,000 samples

Layer 3: stride=2
Outputâ‚ƒ = 12,000 / 2 = 6,000 samples

Final latent code shape: [batch_size, num_channels, 6,000]

Total downsampling = 48,000 / 6,000 = 8x
```

---

### Question 9: Information Theory - Why Compression Possible?

**Q: Use information theory to explain why a neural network can compress audio that seems random to us.**

**A:**

**Audio is NOT random!**

Shannon's Entropy Definition:

```
H(X) = -Î£ p(x) Ã— logâ‚‚(p(x))

Where:
- If all values equally likely: H = maximum
- If some values much more likely: H = lower

Example - 8 values:
Uniform distribution: p(i) = 1/8 for all i
H = -8 Ã— (1/8 Ã— logâ‚‚(1/8)) = 3 bits

Skewed distribution: p(0)=0.5, p(1)=0.25, others=0.0625 each
H = -0.5Ã—logâ‚‚(0.5) - 0.25Ã—logâ‚‚(0.25) - 6Ã—(0.0625Ã—logâ‚‚(0.0625))
  â‰ˆ 2.25 bits (lower!)
```

**Why audio has low entropy:**

```
x[n] depends heavily on x[n-1]:
- Can't jump from -1.0 to +1.0 instantly
- Natural frequencies (bass, mids, treble) have structure
- Music has patterns, repetition

Conditional entropy H(X[n] | X[n-1], X[n-2], ..., X[n-k]):
- Raw samples: H â‰ˆ 16 bits (high!)
- Given previous samples: H â‰ˆ 8 bits (lower)
- Given context: H â‰ˆ 3 bits (much lower)

The encoder learns to use this context!

z[m] = compress(context_from_many_samples)
Encoded: H(z) â‰ˆ 3 bits << H(x) â‰ˆ 16 bits
```

---

### Question 10: Trade-off Parameters

**Q: In the loss function L_total = L_recon + Î²Ã—L_rate + Î³Ã—L_commit:**

- **If Î² is very large**: What happens?
- **If Î² is very small**: What happens?

**A:**

```
If Î² LARGE (Î² â†’ âˆ):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L_total â‰ˆ Î² Ã— L_rate (dominates!)

Optimizer focuses on minimizing bitrate
â†’ Aggressive compression
â†’ Many artifacts in audio (noise, distortion)
â†’ Small file size
â†’ Poor perceptual quality
â†’ Use case: Ultra-low bandwidth (2 kbps)

Extreme: Î²=10.0
Result: 3 kbps, sounds roboticized/compressed
```

```
If Î² SMALL (Î² â†’ 0):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L_total â‰ˆ L_recon (dominates!)

Optimizer focuses on perfect reconstruction
â†’ Gentle compression
â†’ Pristine audio quality
â†’ Large file size
â†’ Perfect fidelity
â†’ Use case: High-quality (256 kbps)

Extreme: Î²=0.001
Result: 256 kbps, sounds perfect
```

**Choosing Î²:**

```
Î² is a hyperparameter you tune:
- Î² = 0.01: High quality, 128 kbps
- Î² = 0.05: Good balance, 48 kbps  
- Î² = 0.1: Aggressive compression, 24 kbps
- Î² = 0.2: Very aggressive, 12 kbps

Rate-distortion curve: you choose where on the curve to operate!
```

---

### Question 11: Codebook Learning

**Q: What's "codebook" and why does the network learn to use fewer codes?**

**A:**

**Codebook:**

```
A set of "favorite" values the network learns:

During training, quantization rounds to nearest integer:
z_q[m] = round(z[m])

But with learning, the network naturally clusters around values:
z_q values used: {-2, -1, 0, 1, 2, 5, 10, ...}

These become the "codebook" - the values it actually uses!
```

**Why fewer codes?**

```
Two benefits:
1. Entropy coding is more efficient with fewer codes
2. Probability distribution becomes peakier

Example:
Bad codebook: uses all 256 possible 8-bit values equally
p(all codes) = 1/256 each
Entropy H = 8 bits (no compression!)

Good codebook: uses only 10 values
p(codeâ‚€)=0.3, p(codeâ‚)=0.2, ..., p(codeâ‚‰)=0.01
Entropy H â‰ˆ 2.5 bits (good compression!)

The network learns:
"These 10 codes capture 99% of what I need to represent audio.
Other codes are wasteful."

Entropy model learns p(z_q) and ignores unused codes!
```

---

### Question 12: Reconstruction Loss Details

**Q: Write out the STFT loss formula with explanation.**

**A:**

**Short-Time Fourier Transform:**

```
X[t, k] = Î£(n=0 to N-1) x[n + tÃ—hop] Ã— window(n) Ã— e^(-j2Ï€kn/N)

Where:
- t = frame index
- k = frequency bin (0 to N-1)
- N = FFT size (e.g., 512, 1024, 2048)
- hop = hop size (e.g., N/4)
- window(n) = Hann window to reduce spectral leakage

Properties:
- X[t, k] = complex number = magnitude + phase
- |X[t, k]| = amplitude (how strong is this frequency)
- âˆ X[t, k] = phase (timing/alignment)
```

**Multi-Scale STFT Loss:**

```
L_STFT = Î£(scales S={512, 1024, 2048, ...}) {

    # Magnitude loss (log scale)
    L_mag(S) = (1 / Î£|X_s|) Ã— Î£ | log|X_orig| - log|X_recon| |
    
    # Phase loss (only where magnitude is significant)
    L_phase(S) = (1 / Î£|X_s|) Ã— Î£ |âˆ X_orig - âˆ X_recon| Ã— W(|X_orig|)
    
    Total_S = L_mag(S) + L_phase(S)
}

Where:
- S = FFT size (multiple scales tested)
- X_orig = STFT of original audio
- X_recon = STFT of reconstructed audio
- W(Â·) = weighting (emphasize where magnitude is high)
- log magnitude: matches human perception (log frequency)
- Multiple scales: captures both details and broad structure
```

**Why this works:**

```
âœ“ Magnitude in log scale: matches how humans hear volume
âœ“ Phase information: preserves transients and timing
âœ“ Multiple scales: 
  - 512: catches high-frequency details
  - 1024: medium frequencies and timing
  - 2048: low frequencies and long-term structure
âœ“ Much better perceptual match than MSE!
```

---

### Question 13: Entropy Model

**Q: What exactly is the entropy model? How is it trained?**

**A:**

**Entropy Model (Separate Neural Network):**

```
Input: quantized codes z_q[m, c]
Output: probability p_model(z_q[m, c])

Architecture (simplified):
Input z_q[m, c]
  â†“
[Fully Connected Layers]
  â†“
[Softmax over all possible code values]
  â†“
Output: p_model âˆˆ [0, 1]
```

**Training:**

```
The entropy model is trained to predict: 
"Given that I see code z_q, how likely is it?"

Loss for entropy model:
L_entropy = -log(p_model(z_q[actual]))

Example:
If actual code is z_q = 5
And model predicts: p(5) = 0.08

Loss = -log(0.08) â‰ˆ 3.64 bits

This means: arithmetic coder needs 3.64 bits to encode this code
```

**Two-stage training:**

```
Stage 1: Train encoder/decoder + entropy model jointly
- Update E, D, p simultaneously
- p learns the distribution of z_q from E

Stage 2: Fix E and D, train p more (optional)
- Fine-tune entropy model
- Compresses better after E and D are stable

Why separate?
- E and D want high reconstruction quality
- p wants to model the distribution accurately
- Training together balances both
```

**Using entropy model:**

```
During compression:
1. Encoder produces z
2. Quantize to z_q
3. Entropy model predicts p(z_q)
4. Arithmetic coder uses p to assign bits:
   bits_needed = -logâ‚‚(p(z_q))

If p(z_q) = 0.3: bits = -logâ‚‚(0.3) â‰ˆ 1.74 bits
If p(z_q) = 0.01: bits = -logâ‚‚(0.01) â‰ˆ 6.64 bits
```

---

### Question 14: Complete Backpropagation Flow

**Q: Trace the gradient flow from loss back to encoder weights.**

**A:**

```
FORWARD PASS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Input: x[n] (audio)
  â†“ (apply encoder E with weights Î¸)
z[m] = E_Î¸(x)
  â†“ (quantize with STE: no param, but gradient passes)
z_q[m] = round(z[m])  (STE: gradient as if identity)
  â†“ (apply decoder D with weights Ï†)
y[n] = D_Ï†(z_q)
  â†“ (compute STFT of y and x)
X = STFT(x)
Y = STFT(y)
  â†“ (compute losses)
L_recon = STFT_loss(X, Y)
L_rate = -logâ‚‚(p_Ïˆ(z_q))
L_commit = ||z - sg[z_q]||Â² + ||sg[z] - z_q||Â²
L_total = L_recon + Î²Ã—L_rate + Î³Ã—L_commit

BACKWARD PASS (Backpropagation):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âˆ‚L_total/âˆ‚L_recon = 1
  â†“ (STFT gradient)
âˆ‚L_total/âˆ‚Y = gradient of STFT loss w.r.t output
  â†“ (decoder gradient)
âˆ‚L_total/âˆ‚z_q = gradient through decoder D
  
  Also:
âˆ‚L_total/âˆ‚L_rate = Î²
  â†“ (entropy loss gradient)
âˆ‚L_total/âˆ‚p = gradient w.r.t probability
  â†“ (entropy model gradient)
âˆ‚L_total/âˆ‚z_q += gradient from entropy (accumulate)
  
  Also:
âˆ‚L_total/âˆ‚z_q += Î³ Ã— (gradient from commit loss)

STE step:
âˆ‚L_total/âˆ‚z = âˆ‚L_total/âˆ‚z_q  (straight-through: treat round as identity)

Encoder gradient:
âˆ‚L_total/âˆ‚Î¸ = (âˆ‚L_total/âˆ‚z) Ã— (âˆ‚z/âˆ‚Î¸)  (chain rule)
  â†“
Through all encoder layers backwards:
Layer N-1, Layer N-2, ..., Layer 1, Layer 0

PARAMETER UPDATES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Î¸_new = Î¸_old - Î± Ã— âˆ‚L_total/âˆ‚Î¸
Ï†_new = Ï†_old - Î± Ã— âˆ‚L_total/âˆ‚Ï†
Ïˆ_new = Ïˆ_old - Î± Ã— âˆ‚L_total/âˆ‚Ïˆ

Where Î± = learning rate
```

**Key insight:** Gradient flows backward through:

1. Loss functions (STFT, entropy, commit)
2. Decoder and STE
3. Into encoder
4. Through convolution layers
5. Updates all encoder weights to minimize loss

---

### Question 15: Real Performance Example

**Q: The paper claims 24 kbps compression with CD-quality sound. What does this mean mathematically?**

**A:**

```
ORIGINAL (CD Quality):
- Sample rate: 44.1 kHz
- Bit depth: 16 bits
- Channels: 2 (stereo)

Bitrate = 44.1k Ã— 16 Ã— 2 = 1,411.2 kbps
File size for 1 minute = 1,411.2 kbps Ã— 60 sec = 84,672 kb â‰ˆ 10.6 MB
```

```
COMPRESSED (24 kbps with Neural Network):
- Latent code rate: ~48,000 codes/minute / 60 sec â‰ˆ 800 codes/sec
- Average entropy: -logâ‚‚(p) â‰ˆ 3 bits/code
- Bitrate = 800 codes/sec Ã— 3 bits/code = 2,400 bits/sec â‰ˆ 24 kbps
- File size for 1 minute = 24 Ã— 60 = 1,440 kb â‰ˆ 0.18 MB
```

```
COMPRESSION RATIO:
1,411.2 / 24 â‰ˆ 58.8x compression

But human perception test:
- Original and compressed: ~92% of listeners can't tell difference
- This is "transparent" compression!

Why possible?
- Encoder learns which frequencies humans care about
- Entropy model exploits non-uniform distribution
- STFT loss trains on perceptually relevant features
- Network removes information no human ear can hear
```

```
Comparison with traditional codec (Opus 24 kbps):
- Neural: 24 kbps, 92% transparent
- Opus: 24 kbps, ~85% transparent
Neural codec is better at low bitrates!
```

---

## Summary: The Complete Picture

```
Architecture Overview:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Encoder    â”‚â”€â”€â”€â”€â–¶â”‚   Quantizer  â”‚â”€â”€â”€â”€â–¶â”‚  Entropy     â”‚
â”‚  (learns to  â”‚     â”‚  (rounds to  â”‚     â”‚  Model       â”‚
â”‚  extract     â”‚     â”‚  integers)   â”‚     â”‚  (learns     â”‚
â”‚  key info)   â”‚     â”‚              â”‚     â”‚  p(z_q))     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘                                           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  (probability guides encoder)

Training Objective:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Minimize:                                       â”‚
â”‚ L = L_recon + Î²Ã—L_rate + Î³Ã—L_commit             â”‚
â”‚                                                 â”‚
â”‚ Subject to:                                     â”‚
â”‚ - Reconstruction sounds natural                 â”‚
â”‚ - File compresses efficiently                   â”‚
â”‚ - Codes stay close to quantized values          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Information Flow:
Original Audio (16 bits, high entropy Hâ‰ˆ16)
    â†“ [Encoder learns structure]
Latent Code (floating point, lower entropy Hâ‰ˆ5)
    â†“ [Quantization removes decimals]
Discrete Codes (integers, entropy still â‰ˆ5)
    â†“ [Entropy coding exploits non-uniformity]
Bitstream (3 bits average per code, Hâ‰ˆ3)
    â†“ [Transmission or storage]
At Destination:
    â†“ [Entropy decode]
Discrete Codes
    â†“ [Decoder network]
Reconstructed Audio (perceptually identical)
```

This explains why neural audio compression works so well! ğŸµ